lamorel_args:
  log_level: info
  llm_configs:
    main_llm:
      handler: hf_llm
      constructor_kwargs:
      model_type: causal
      model_path: EleutherAI/pythia-70m #/home/cromac/Documents/TMP/Qwen2.5-Coder-0.5B-bnb-4bit
      pretrained: true
      minibatch_size: 256
      pre_encode_inputs: false
      load_in_4bit: false
      synchronize_gpus_after_scoring: false
      empty_cuda_cache_after_scoring: false
  distributed_setup_args:
    backend: gloo  # nccl
    init_timeout: 120
    timeout: 1800
    multinode: false
    multinode_args:
      main_process_ip: 127.0.0.1
      main_process_port: 25000
      experiment_id: 0
    n_rl_processes: 1
    llm_processes:
      main_llm:
        n_processes: 1
        devices_per_process: [ "cpu"]  # either "cpu" or list of gpu ids
        ddp_kwargs:
rl_script_args:
  path: ???
  pad_contexts: true